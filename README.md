# LLM-Evaluation-Framework

A comprehensive framework for evaluating and comparing different Large Language Models (LLMs) using multiple metrics including relevancy, faithfulness, and hallucination detection. Built with Python and integrating with Groq's API.

#### Features
ğŸ” Multi-model comparison (Gemma, Llama, Mixtral)

ğŸ“Š Three key evaluation metrics:
    
    Relevancy scoring
    
    Faithfulness assessment
    
    Hallucination detection
    
ğŸ” Reason tracking

Model Performance Metrics
    
    Relevancy: Measures how well responses address the questions
    
    Faithfulness: Evaluates alignment with provided context
    
    Hallucination: Detects fabricated information
