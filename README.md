# LLM-Evaluation-Framework

A comprehensive framework for evaluating and comparing different Large Language Models (LLMs) using multiple metrics including relevancy, faithfulness, and hallucination detection. Built with Python and integrating with Groq's API.

#### Features
🔍 Multi-model comparison (Gemma, Llama, Mixtral)

📊 Three key evaluation metrics:
    
    Relevancy scoring
    
    Faithfulness assessment
    
    Hallucination detection
    
🔍 Reason tracking

Model Performance Metrics
    
    Relevancy: Measures how well responses address the questions
    
    Faithfulness: Evaluates alignment with provided context
    
    Hallucination: Detects fabricated information
